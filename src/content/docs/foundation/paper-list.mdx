---
title: 'Paper List'
description: 'A curated collection of influential research papers in robotics, computer vision, and machine learning'
order: 2
updatedDate: 2025-08-10
author: "ACondaway"
authorAvatar: "https://avatars.githubusercontent.com/u/115391544?v=4"
---
import { Card } from 'astro-pure/user'
import { Timeline } from 'astro-pure/user'
import { Aside } from 'astro-pure/user'

<Aside type="tip">
点击card可以导航到对应的paper页面。
</Aside>

## OpenVLA系列工作

<Card
  as='a'
  href='https://arxiv.org/pdf/2406.09246'
  heading='OpenVLA: An Open-Source Vision-Language-Action Model'
  subheading='具身操作VLA foundation model'
  date='2024-06-01'
>
<img src="/src/content/docs/foundation/papers/OpenVLA.png" alt="OpenVLA" />
</Card>
<br />
<Card
  as='a'
  href='https://arxiv.org/pdf/2502.19645'
  heading='Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success'
  subheading='具身操作VLA foundation model'
  date='2025-02-01'
>
<img src="/src/content/docs/foundation/papers/openvla-oft.png" alt="OpenVLA-OFT" />
</Card>
<br />

## RDT系列工作
<Card
  as='a'
  href='https://arxiv.org/pdf/2410.07864'
  heading='RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation'
  subheading='双臂协同操作foundation model'
  date='2024-10-01'
>
<img src="/src/content/docs/foundation/papers/RDT.png" alt="RDT" />
</Card>
<br />
<Card
  as='a'
  href='https://www.arxiv.org/pdf/2507.23523'
  heading='H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation'
  subheading='面向更加数据高效的双臂协同操作foundation model'
  date='2024-10-01'
>
<img src="/src/content/docs/foundation/papers/HRDT.png" alt="H-RDT" />
</Card>
## TikTok GR系列工作

<Card
  as='a'
  href='https://arxiv.org/pdf/2312.13139'
  heading='UNLEASHING LARGE-SCALE VIDEO GENERATIVE PRE-TRAINING FOR VISUAL ROBOT MANIPULATION'
  subheading='字节跳动提出的基于大规模视频预训练模型'
  date='2023-12-01'
>
<img src="/src/content/docs/foundation/papers/GR1.png" alt="GR-1" />
</Card>
<br />
<Card
  as='a'
  href='https://arxiv.org/pdf/2410.06158'
  heading='GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation'
  subheading='字节跳动提出的基于大规模视频预训练模型'
  date='2024-10-01'
>
<img src="/src/content/docs/foundation/papers/GR2.png" alt="GR-2" />
</Card>
<br />
<Card
  as='a'
  href='https://arxiv.org/html/2507.15493v1'
  heading='GR-3 Technical Report'
  subheading='字节跳动提出的基于大规模视频预训练模型'
  date='2025-07-01'
>
<img src="/src/content/docs/foundation/papers/GR3.png" alt="GR-3" />
</Card>
<br />

## RT系列工作

<Card
  as='a'
  href='https://arxiv.org/pdf/2212.06817'
  heading='RT-1: Robotics Transformer for Real-World Control at Scale'
  subheading='RT系列VLA关键工作'
  date='2024-10-01'
>
<img src="/src/content/docs/foundation/papers/RT1.png" alt="RT-1" />
</Card>
<br />
<Card
  as='a'
  href='https://arxiv.org/pdf/2307.15818'
  heading='RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control'
  subheading='RT系列VLA关键工作'
  date='2023-07-01'
>
<img src="/src/content/docs/foundation/papers/RT2.png" alt="RT-2" />
</Card>
<br />



## π系列工作

<Card
  as='a'
  href='https://arxiv.org/pdf/2410.24164v1'
  heading='π0: A Vision-Language-Action Flow Model for General Robot Control'
  subheading='PI系列VLA关键工作'
  date='2024-10-01'
>
<img src="/src/content/docs/foundation/papers/pi0.png" alt="π0" />
</Card>
<br />
<Card
  as='a'
  href='https://arxiv.org/pdf/2410.24164v1'
  heading='π0.5: a Vision-Language-Action Model with Open-World Generalization'
  subheading='PI系列VLA关键工作'
  date='2024-10-01'
>
<img src="/src/content/docs/foundation/papers/pi05.png" alt="π0.5" />
</Card>
<br />

*Last updated: Aug.10 2025*